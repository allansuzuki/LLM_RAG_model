{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMeHg2SWeEWZ3hc2rJdFVzn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/allansuzuki/LLM_RAG_model/blob/main/treino_llm_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[![author](https://img.shields.io/badge/author-allansuzuki-red.svg)](https://www.linkedin.com/in/allanysuzuki/) [![](https://img.shields.io/badge/python-3.9-blue.svg)](https://www.python.org/downloads/release/python-365/) [![MIT license](https://img.shields.io/badge/License-MIT-yellow.svg)](http://perso.crans.org/besson/LICENSE.html) [![contributions welcome](https://img.shields.io/badge/contributions-welcome-brightgreen.svg?style=flat)](https://github.com/allansuzuki/LLM_RAG_model/issues)"
      ],
      "metadata": {
        "id": "cxrUq0YhmjTd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Following langchain quickstart in\n",
        "* https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa\n",
        "* https://python.langchain.com/docs/guides/local_llms\n",
        "\n",
        "About langchain chains (LLMchain):\n",
        "https://python.langchain.com/docs/modules/chains\n",
        "\n",
        "and support from Mario Silva project to use LLAMA2 local model importing from huggingface:\n",
        "* https://github.com/MarioCSilva/RAG_QA_LLM/blob/main/llama2_RAG_QA.ipynb"
      ],
      "metadata": {
        "id": "_eQ_DoOy_sxf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Have you ever imagine a way to give more context about a subject to a LLM model in a easy way, helping them narrowing to something more specific and even give more details to enhance the answer properly?\n",
        "\n",
        "That's the main purpose on RAG (Retrieval-Augmented Generation): no need on train the model on new (and maybe particular) data, returning satisfactory answer based on provided documents,texts, ...\n",
        "\n",
        "In this use case we are going to use langchain framework to build the prompts and use the LLM models to answer a documentation question.\n",
        "\n",
        "Langchain uses simple prompt parsers to work on multiple LLM, integrated with Meta llama2-chat model through HuggingFace pipeline.\n",
        "\n"
      ],
      "metadata": {
        "id": "n9dP37dcP0NX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install"
      ],
      "metadata": {
        "id": "8K0Ry_9rJzn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# necessary to install requirements properly\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\""
      ],
      "metadata": {
        "id": "9uE1i7TQlpIS"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FZ_0G5Da_h1V"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade --quiet langchain langchain-community chromadb bs4 huggingface_hub\n",
        "!pip install -U -q transformers accelerate bitsandbytes sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# or get the requirements !pip freeze > requirements.txt\n",
        "# and install based on requirements.txt !pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "Los55hCUJ3wk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set model to load"
      ],
      "metadata": {
        "id": "qdUbPhzOD0Db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we get the Hugging Face API token and the Hugging Face model_id"
      ],
      "metadata": {
        "id": "eQGDsVMFQgMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# get access tokens here -- need to add in the key icon on the left tab with `NAME` as variable name and `VALUE` with the secret value\n",
        "# In this case, I added my Hugging Face token in the HF_TOKEN colab secrets\n",
        "from google.colab import userdata\n",
        "hf_auth = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "c2uyXwQzopkt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# get the model id on hugging face\n",
        "model_id = 'meta-llama/Llama-2-7b-chat-hf'"
      ],
      "metadata": {
        "id": "CTz1xkgWESNi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "\n",
        "# get device for torch to run on GPU\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "5gHOnv-2Jqr0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization"
      ],
      "metadata": {
        "id": "ZN_GQX7WOF9W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Quantization in the context of Large Language Models (LLMs) refers to the process of converting the weights of a model from higher precision data types to lower-precision ones. This makes complex and heavier models to be loaded in less memory RAM.\n",
        "\n",
        "Where are going to use BitsAndBytes to apply the quantization."
      ],
      "metadata": {
        "id": "Hwrg-MCXQ-9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set quantization configuration to load large model with less GPU memory\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "LgepZMXTNM7u"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # how the quantization config looks like\n",
        "# display(bnb_config)"
      ],
      "metadata": {
        "id": "Z4SglAo0Nptk"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model configurations"
      ],
      "metadata": {
        "id": "SG59rZTeOJpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It loads the model weights and configurations from Hugging Face"
      ],
      "metadata": {
        "id": "gsHHRVpQSCRy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# begin initializing HF items with hf auth token\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "1z1qo2X7NU_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # how the model config looks like\n",
        "# display(model_config)"
      ],
      "metadata": {
        "id": "05NiuL5_OvXR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load model"
      ],
      "metadata": {
        "id": "pQJJSd2sOSH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It loads the LLM model through transformers pipeline, using the quantization and model configs. Device_map accelerates the model performance when possible."
      ],
      "metadata": {
        "id": "WkelG_9NSK6R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load HF model\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "# # enable evaluation mode to allow model inference -- no need for now\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "lBn_2pAJOQeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model loaded on {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf7nzEuTNao_",
        "outputId": "d1519707-48e0-421f-ab82-71db3896e563"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load tokenizer"
      ],
      "metadata": {
        "id": "2crl2VFhvbxG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenizing is the process of converting human words in numbers mapped from a tokenizer, transforming into `tokens`.\n",
        "\n",
        "For example, the phrase \"Hello world\" could be (0.93844 0.4323 0.9434 -0.3252) after tokenizing. It allows better understading and usage of words into LLM models"
      ],
      "metadata": {
        "id": "JGh-HAvVS6yC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "\n",
        "# settings: model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ],
      "metadata": {
        "id": "GciyZki8vddi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop on tokens"
      ],
      "metadata": {
        "id": "JJgZM5_wwOsU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We create the process of stop on tokens to stop the model continuing generating text when step into these words, controlling model's output.\n",
        "\n",
        "We are using tokenizer to convert the stop words in tokens and torch.LongTensor to store values with high precision."
      ],
      "metadata": {
        "id": "5TENoFrlUFH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stop_list = ['\\nHuman:', '\\n```\\n']\n",
        "\n",
        "# stop_token_ids in list\n",
        "stop_token_ids = [tokenizer(x)['input_ids'] for x in stop_list]\n",
        "\n",
        "# display a sample\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2GzRF5rveuq",
        "outputId": "3f2e5f83-c1e4-4cf1-c0f2-231992018d7b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 29871, 13, 29950, 7889, 29901], [1, 29871, 13, 28956, 13]]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import LongTensor\n",
        "\n",
        "# stop_token_ids in tensor\n",
        "stop_token_ids = [LongTensor(x).to(device) for x in stop_token_ids]\n",
        "stop_token_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqMI5wLSwZIr",
        "outputId": "2288a31c-7799-4863-8629-8957734a1bd1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([    1, 29871,    13, 29950,  7889, 29901], device='cuda:0'),\n",
              " tensor([    1, 29871,    13, 28956,    13], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import StoppingCriteria, StoppingCriteriaList\n",
        "from torch import LongTensor, FloatTensor, eq\n",
        "\n",
        "# define custom stopping criteria object\n",
        "class StopOnTokens(StoppingCriteria):\n",
        "    def __call__(self, input_ids: LongTensor, scores: FloatTensor, **kwargs) -> bool:\n",
        "        for stop_ids in stop_token_ids:\n",
        "            if eq(input_ids[0][-len(stop_ids):], stop_ids).all():\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "stopping_criteria = StoppingCriteriaList([StopOnTokens()])"
      ],
      "metadata": {
        "id": "9VXKu2Ktw2E9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load documents"
      ],
      "metadata": {
        "id": "S8efsRKj8w7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine we want to know more about LLM Agents, but not only using generalized knowledge, but focused on a particular post in https://lilianweng.github.io\n",
        "\n",
        "We use WebBaseLoader and beatiful-soup to scrape data from the website post."
      ],
      "metadata": {
        "id": "DaE5sSylWdT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "# Only keep post title, headers, and content from the full HTML.\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "mYBZ97kW80e8"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample on document retrieved\n",
        "print('docs sample:\\nlen:',len(docs[0].page_content),'\\n-----',docs[0].page_content[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7s1qKch-rhD",
        "outputId": "db4c2c4e-ce7c-4ee3-a33c-f51d7326e51b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs sample:\n",
            "len: 42824 \n",
            "----- \n",
            "\n",
            "      LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split documents"
      ],
      "metadata": {
        "id": "HDnjYkCU-Bn4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it comes many different strategies to feed the information to the model without overwhelm its memory with unnused information.\n",
        "\n",
        "The first thing we are goint to do is to use `RecursiveCharacterSplitter` to split the docuemnt in continuos chunks, overlapping a little bit to avoid missing context."
      ],
      "metadata": {
        "id": "JtznxUUlXQca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200, add_start_index=True    # not clear what \"start_index\" means\n",
        ")\n",
        "all_splits = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "l8GsQUuZ-mEF"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample on how the document was splitted\n",
        "print(actual_doc := all_splits[0].page_content,all_splits[0].metadata,'\\nlen:',len(actual_doc),sep='\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Y5ktZZn_qNC",
        "outputId": "80d476b7-2514-4c3d-f14f-cb3a8fdcdb2e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LLM Powered Autonomous Agents\n",
            "    \n",
            "Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\n",
            "\n",
            "\n",
            "Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\n",
            "Agent System Overview#\n",
            "In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\n",
            "\n",
            "Planning\n",
            "\n",
            "Subgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\n",
            "Reflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\n",
            "\n",
            "\n",
            "Memory\n",
            "{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 8}\n",
            "\n",
            "len:\n",
            "969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings"
      ],
      "metadata": {
        "id": "W2ndl3KBLTL_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we want to capture the meaning of the sentence. For this we rely on `Embeddings` because it assigns a code (a.k.a embedding) to a sequence of words. It creates an embbeding mapping (or space), which makes the words with similiar meanings closer to each other.\n",
        "\n",
        "We are going to apply this Embedding transform from Hugging Face Hub."
      ],
      "metadata": {
        "id": "VIk5Sgz2YH2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "hf_embedding = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\",\n",
        "    model_kwargs={\"device\": \"cuda\"}\n",
        "  )"
      ],
      "metadata": {
        "id": "d_iSg5a0HcsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector store"
      ],
      "metadata": {
        "id": "8g5O29nfBwzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have the document splitted and the words' embedding, we will store all these sequences and meanings in a vector, that can be easily accessed later for fast queries on relevant documents.\n",
        "\n",
        "We are using `Chroma` to create this vector store from the documents we gathered."
      ],
      "metadata": {
        "id": "yN5Zd89JacxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "vectorstore = Chroma.from_documents(documents=all_splits, embedding=hf_embedding)"
      ],
      "metadata": {
        "id": "IjhQE2FbF5n5"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# that's how the first split looks like in our vector store\n",
        "first_vector = vectorstore.get()['ids'][0]\n",
        "vectorstore.get(first_vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gZ4Y9bWDik0",
        "outputId": "7ae43bf0-cde7-4cb8-f651-bf71e5b365a2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': ['a236b272-c47a-11ee-b55b-0242ac1c000c'],\n",
              " 'embeddings': None,\n",
              " 'metadatas': [{'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',\n",
              "   'start_index': 8}],\n",
              " 'documents': ['LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'],\n",
              " 'uris': None,\n",
              " 'data': None}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# that's how the text looks like for the model\n",
        "print(hf_embedding.embed_documents(vectorstore.get(first_vector)['documents'][0].split('\\n\\n'))[0][:5],'...')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFTYSA-WVvv8",
        "outputId": "09ae1e8d-13cb-4aea-eb93-c537712fedff"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.014027159661054611, 0.007987729273736477, 0.005489276722073555, -0.016271350905299187, -0.02559383027255535] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Cases on document search or QA"
      ],
      "metadata": {
        "id": "tIc9H-GgbfsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Query relevant documents using Retriever (QA wo LLM)"
      ],
      "metadata": {
        "id": "IJQzTqkyoUzo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we see the results the retriever does, searching in all the information provided. Since the retriever returns the most similar text based on the query and we know these documents contains the answer, it's a reliable answer, but not contextualized."
      ],
      "metadata": {
        "id": "p4d8dEjppSbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# settings for retriever\n",
        "similarity_top_k = 6\n",
        "mmr_threshold = 0.7\n",
        "search_kwargs = {\n",
        "    'score_threshold' : mmr_threshold,\n",
        "    'k' : similarity_top_k\n",
        "    }\n",
        "\n",
        "retriever = vectorstore.as_retriever(search_type='mmr', search_kwargs = search_kwargs)"
      ],
      "metadata": {
        "id": "RaMIA4K6oV_g"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example how it returns\n",
        "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")"
      ],
      "metadata": {
        "id": "NPL-PPYNr7hB"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('docs retrieved:', len(retrieved_docs),'\\nsample content',retrieved_docs[0].page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyp8ritXsBH5",
        "outputId": "a733ef03-773c-4f76-8e53-8115a7a417a9"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "docs retrieved: 6 \n",
            "sample content Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer using LLM text generation without RAG"
      ],
      "metadata": {
        "id": "klJVD2UhcVTN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "#build the template to prompt\n",
        "template = \"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, don't make the answer and say you don't know. Use three sentences maximum and keep the answer concise.\n",
        "Question: {question}\n",
        "Context: {context}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# build prompt\n",
        "prompt = PromptTemplate.from_template(template)"
      ],
      "metadata": {
        "id": "2QWJZX6uE8z3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# example of message\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"all possible context here\", \"question\": \"your question here\"}\n",
        ").to_messages()\n",
        "\n",
        "print(example_messages[0].content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqjvYzlSHWQr",
        "outputId": "db9325f9-70ca-4ef3-9d49-4333502a4e19"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, don't make the answer and say you don't know. Use three sentences maximum and keep the answer concise.\n",
            "Question: your question here\n",
            "Context: all possible context here\n",
            "Answer:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we see the results of a LLM model try to answer wihtout any provided document as context. We see it gives explanation, samples, but since it does not know what we are interest in, so it does not fulfill our needs."
      ],
      "metadata": {
        "id": "ozEefQxzpGEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "temperature = 0.2\n",
        "max_new_tokens = 512\n",
        "repetition_penalty = 1.1\n",
        "\n",
        "transformers_pipeline = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "    stopping_criteria=stopping_criteria,  # without this model rambles during chat\n",
        "    temperature=temperature,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=max_new_tokens,  # max number of tokens to generate in the output\n",
        "    repetition_penalty=repetition_penalty  # without this output begins repeating\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=transformers_pipeline)"
      ],
      "metadata": {
        "id": "6B7EAVUVxGAF"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_wo_rag = llm.invoke('What is Task Decomposition?')"
      ],
      "metadata": {
        "id": "dVJ9O5KmxZzL"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LLM response\n",
        "output_wo_rag"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "6IYHDy8GjyIt",
        "outputId": "81e3f44a-d08a-49bf-c2a6-64d61844a72c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n nobody likes to do tasks that are too big or overwhelming. So, how can we break down these tasks into smaller, more manageable pieces? This is where task decomposition comes in. Task decomposition is the process of breaking down a large task into smaller, more manageable sub-tasks. By doing so, you can make the task feel less overwhelming and more achievable. In this article, we will explore what task decomposition is, why it\\'s important, and how to apply it to your work and personal life. What is Task Decomposition? Task decomposition is the process of breaking down a complex task into smaller, more manageable parts. It involves identifying the individual steps or components of the task and then prioritizing and organizing them in a way that makes sense for the project or task at hand. The goal of task decomposition is to create a clear and actionable plan that can help you complete the task more efficiently and effectively. Why is Task Decomposition Important? Task decomposition is important because it helps you to: 1. Manage complexity: Large tasks can be overwhelming and difficult to manage. By breaking them down into smaller parts, you can make them feel less daunting and more manageable. 2. Identify key steps: By breaking down a task into smaller parts, you can identify the most critical steps and focus on completing those first. 3. Prioritize tasks: When you have a large task to complete, it can be challenging to know where to start. Task decomposition helps you to prioritize the most important tasks and tackle them first. 4. Create a plan of action: Once you have broken down the task into smaller parts, you can create a plan of action that outlines the steps you need to take to complete each part. This can help you stay organized and focused throughout the task completion process. How to Apply Task Decomposition in Your Work and Personal Life Here are some practical tips for applying task decomposition in your work and personal life: 1. Start by identifying the task you want to break down. This could be a project at work, a personal goal, or a task that needs to be completed around the house. 2. Ask yourself, \"What are the individual steps involved in completing this task?\" Try to identify as many details as possible. 3. Prioritize the steps based on their importance and urgency. Focus on comple'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Answer using RAG and LLM text generation"
      ],
      "metadata": {
        "id": "pnR6T-JFIAGn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see the last case, the model was able to answer but with a generalized long answer. The ideal would be having this kind of humanized answer + context about the subject. That's where the RAG feature takes part on improving the model."
      ],
      "metadata": {
        "id": "bULjTnDWdylF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt, output_key='response')"
      ],
      "metadata": {
        "id": "ThgG0DXOdIzf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = chain({\n",
        "    'question': (question := 'What is Task Decomposition?'),\n",
        "    'context': retriever.invoke(question)\n",
        "})"
      ],
      "metadata": {
        "id": "QLjtvp0AHPYE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_response(output):\n",
        "    print(f\"Q: {output['question']}\",\n",
        "          f\"\\nA:{output['response']}\\n\",\n",
        "          '-'*100,\n",
        "          'DOCUMENTS SEARCHED:',\n",
        "          '\\n'.join(\n",
        "              [f'--{i+1}:\\nlink:{cont.metadata[\"source\"]}\\n{cont.page_content}\\n' for i,cont in enumerate(output['context'])]\n",
        "              ),\n",
        "          sep='\\n')"
      ],
      "metadata": {
        "id": "Xq-BI6O5nTX9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have the perfect case which we have a LLM model answering things more humanized and narrowing to a more contextualized answer."
      ],
      "metadata": {
        "id": "9tVUP6kXebDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pretty_print_response(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sfw8-nW8krHC",
        "outputId": "f9b30a58-32d3-4393-b4b0-530573f15746"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What is Task Decomposition?\n",
            "\n",
            "A: Task decomposition is the process of breaking down a complex task into smaller, more manageable parts. This can involve identifying the individual steps involved in completing the task, as well as determining the dependencies between those steps. By decomposing a task in this way, it becomes easier to understand and execute the task, as well as to identify any potential challenges or obstacles that may arise during completion.\n",
            "\n",
            "----------------------------------------------------------------------------------------------------\n",
            "DOCUMENTS SEARCHED:\n",
            "--1:\n",
            "link:https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\n",
            "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n",
            "\n",
            "--2:\n",
            "link:https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\n",
            "The system comprises of 4 stages:\n",
            "(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\n",
            "Instruction:\n",
            "\n",
            "--3:\n",
            "link:https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can't be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.\n",
            "\n",
            "--4:\n",
            "link:https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Resources:\n",
            "1. Internet access for searches and information gathering.\n",
            "2. Long Term memory management.\n",
            "3. GPT-3.5 powered Agents for delegation of simple tasks.\n",
            "4. File output.\n",
            "\n",
            "Performance Evaluation:\n",
            "1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\n",
            "2. Constructively self-criticize your big-picture behavior constantly.\n",
            "3. Reflect on past decisions and strategies to refine your approach.\n",
            "4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.\n",
            "\n",
            "--5:\n",
            "link:https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Fig. 5. After fine-tuning with CoH, the model can follow instructions to produce outputs with incremental improvement in a sequence. (Image source: Liu et al. 2023)\n",
            "The idea of CoH is to present a history of sequentially improved outputs  in context and train the model to take on the trend to produce better outputs. Algorithm Distillation (AD; Laskin et al. 2023) applies the same idea to cross-episode trajectories in reinforcement learning tasks, where an algorithm is encapsulated in a long history-conditioned policy. Considering that an agent interacts with the environment many times and in each episode the agent gets a little better, AD concatenates this learning history and feeds that into the model. Hence we should expect the next predicted action to lead to better performance than previous trials. The goal is to learn the process of RL instead of training a task-specific policy itself.\n",
            "\n",
            "--6:\n",
            "link:https://lilianweng.github.io/posts/2023-06-23-agent/\n",
            "Fig. 8. Categorization of human memory.\n",
            "We can roughly consider the following mappings:\n",
            "\n",
            "Sensory memory as learning embedding representations for raw inputs, including text, image or other modalities;\n",
            "Short-term memory as in-context learning. It is short and finite, as it is restricted by the finite context window length of Transformer.\n",
            "Long-term memory as the external vector store that the agent can attend to at query time, accessible via fast retrieval.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "-----\n"
      ],
      "metadata": {
        "id": "LBT-oBSJhDLJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see the differences in use cases using LLMs and Retrivers, but also the one we highlighted in this exercise, which combine these two powerful tools in one, to make, for example, a more accurate QA bot or document search engine."
      ],
      "metadata": {
        "id": "Z7VvJGBxhGxp"
      }
    }
  ]
}